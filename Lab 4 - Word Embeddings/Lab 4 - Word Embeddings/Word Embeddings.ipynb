{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5998a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Your full names\n",
    "Student_1 = \"Yasmin Ashraf Hassan Ghanem\"\n",
    "Student_2 = \"Yasmin Abdullah Nasser Saeed Mahmoud Elgendi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858b2ed",
   "metadata": {},
   "source": [
    "# Word Embeddings Assignment\n",
    "We learnt about how to train word embeddings and how these embeddings can represent the meaning of words. Of course you don't need to train your word embeddings model every time you need to use them. In this assignment, we will use pre-trained word embeddings to see some real world usages of them. There are two types of questions in this assignment:\n",
    "1. Coding questions: where you will write normal code\n",
    "2. Essay questions: where you will need to write a sentence to answer the provided questions so don't forget to write the answers for this type\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "# Predict the country from its capital\n",
    "In this assignment, our main goal is to use word embeddings to predict the country given its capital just by applying some vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3162cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f04faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('capitals.txt', delimiter=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
    "\n",
    "# print first five elements in the DataFrame\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53389420",
   "metadata": {},
   "source": [
    "## Loading the word embeddings\n",
    "We will work with google news word embedding dataset but the original data is about 3.4 GigaBytes. To remove this problem, You will only need to work with the provided file `word_embeddings_subset.p` that contains a very small subset of the original embeddings that we will need to use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f132f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\n",
    "print(len(word_embeddings))\n",
    "print(word_embeddings['Spain'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be33643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## TODO: Answer the following questions #####################################\n",
    "# Q1: What does the 243 printed above represent?\n",
    "A1 = \"\"\"\n",
    "The size of the word embeddings 243 words embedded (vocab)\n",
    "\"\"\"\n",
    "\n",
    "# Q2: What does the (300,) printed above represent?\n",
    "A2 = \"\"\"\n",
    "Dimensions of the word embedding of \"Spain\" which is 300 meaning that it is a column vector\n",
    "\"\"\"\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d955c3",
   "metadata": {},
   "source": [
    "## Compute the cosine similarity\n",
    "The cosine similarity between two vectors:\n",
    "\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53d22a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    cos = None\n",
    "    ############################# TODO: Compute the cosine similarity ####################################\n",
    "    normA = np.linalg.norm(A)\n",
    "    normB = np.linalg.norm(B)\n",
    "    cos = np.dot(A, B) / (normA*normB)\n",
    "    ######################################################################################################\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21f97520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510956\n"
     ]
    }
   ],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "print(cosine_similarity(king, queen))\n",
    "assert cosine_similarity(king, queen) - 0.6510956 < 1e-6, \"Cosine Similarity Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ec5a9",
   "metadata": {},
   "source": [
    "## Finding the country of each capital\n",
    "\n",
    "Now, you  will use the previous function to compute similarities between vectors,\n",
    "and use these to find the capital cities of countries. You will write a function that\n",
    "takes in three words, and the embeddings dictionary. Your task is to find the country of \n",
    "capital cities. For example, given the following words: \n",
    "\n",
    "- 1: Athens 2: Greece 3: Baghdad,\n",
    "\n",
    "your task is to predict the country 4: Iraq.\n",
    "\n",
    "**Instructions**: \n",
    "\n",
    "1. To predict the country of the capital you might want to look at the *King - Man + Woman = Queen* example, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\n",
    "\n",
    "2. Iterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\n",
    "\n",
    "3. You should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd5c09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1: a string (the capital city of country1)\n",
    "        country1: a string (the country of capital1)\n",
    "        city2: a string (the capital city of country2)\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        country: a tuple containing the word and the similarity score\n",
    "    \"\"\"\n",
    "    ################################## TODO: Implement the following lines ######################################\n",
    "\n",
    "    # store the city1, country 1, and city 2 in a set called group\n",
    "    group = set((city1, country1, city2))\n",
    "\n",
    "    # get embeddings of city 1\n",
    "    city1_emb = embeddings[city1]\n",
    "\n",
    "    # get embedding of country 1\n",
    "    country1_emb = embeddings[country1]\n",
    "\n",
    "    # get embedding of city 2\n",
    "    city2_emb = embeddings[city2]\n",
    "\n",
    "    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n",
    "    # Remember: King - Man + Woman = Queen\n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "\n",
    "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
    "    similarity = -1\n",
    "\n",
    "    # initialize country to an empty string\n",
    "    country = ''\n",
    "\n",
    "    # loop through all words in the embeddings dictionary\n",
    "    for word in embeddings.keys():\n",
    "\n",
    "        # first check that the word is not already in the 'group'\n",
    "        if word not in group:\n",
    "\n",
    "            # get the word embedding\n",
    "            word_emb = embeddings[word]\n",
    "\n",
    "            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n",
    "            cur_similarity = cosine_similarity(vec, word_emb)\n",
    "\n",
    "            # if the cosine similarity is more similar than the previously best similarity...\n",
    "            if cur_similarity > similarity:\n",
    "\n",
    "                # update the similarity to the new, better similarity\n",
    "                similarity = cur_similarity\n",
    "\n",
    "                # store the country as a tuple, which contains the word and the similarity\n",
    "                country = (word, similarity)\n",
    "\n",
    "    #########################################################################################################\n",
    "\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5665d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egypt\n",
      "0.7626821\n"
     ]
    }
   ],
   "source": [
    "# Testing your function\n",
    "country_test, similarity_test = get_country('Athens', 'Greece', 'Cairo', word_embeddings)\n",
    "print(country_test, similarity_test, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d291c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### TODO: Answer The following question #############################\n",
    "# Q3: Is the country predicted above is what you expected?\n",
    "A3 = \"\"\"\n",
    "    Yes\n",
    "\"\"\"\n",
    "\n",
    "# Q4: Does the similarity printed above make sense? Why?\n",
    "A4 = \"\"\"\n",
    "    Yes. \n",
    "    This is because it is near 1. and indeed Egypt and Cairo are similar words in meaning just as athens and Greece are similar\n",
    "\"\"\"\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dd78e",
   "metadata": {},
   "source": [
    "## Model Accuracy\n",
    "\n",
    "Now you will test your new function on the dataset and check the accuracy of the model:\n",
    "\n",
    "$$\\text{Accuracy}=\\frac{\\text{Correct # of predictions}}{\\text{Total # of predictions}}$$\n",
    "\n",
    "**Instructions**: Write a program that can compute the accuracy on the dataset provided for you. You have to iterate over every row to get the corresponding words and feed them into you `get_country` function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0336f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "\n",
    "    ################################ TODO: Implement this function ####################################\n",
    "    # initialize num correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through the rows of the dataframe\n",
    "    for i, row in data.iterrows():\n",
    "\n",
    "        # get city1\n",
    "        city1 = row['city1']\n",
    "\n",
    "        # get country1\n",
    "        country1 = row['country1']\n",
    "\n",
    "        # get city2\n",
    "        city2 =  row['city2']\n",
    "\n",
    "        # get country2\n",
    "        country2 = row['country2']\n",
    "\n",
    "        # use get_country to find the predicted country2\n",
    "        predicted_country2, _ = get_country(city1, country1, city2, word_embeddings)\n",
    "\n",
    "        # if the predicted country2 is the same as the actual country2...\n",
    "        if predicted_country2 == country2:\n",
    "            # increment the number of correct by 1\n",
    "            num_correct += 1\n",
    "\n",
    "    # get the number of rows in the data dataframe (length of dataframe)\n",
    "    m = len(data)\n",
    "\n",
    "    # calculate the accuracy by dividing the number correct by m\n",
    "    accuracy = num_correct/m\n",
    "\n",
    "    ##################################################################################################\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa3fd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "# This cell may take about 30 seconds to run so don't worry\n",
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86043ef",
   "metadata": {},
   "source": [
    "# Plotting the vectors using PCA\n",
    "\n",
    "Now you will explore the distance between word vectors after reducing their dimension. You should have known above the dimensionality of the word embeddings. Of course we can't visualize such a high dimension of vectors so we need to use some dimensionality reduction technique to reduce the dimensions to 2 so we can visualize them. Since you studied before PCA, we will use it for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8583b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def compute_pca(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m,n) where each row corresponds to a word vector\n",
    "        n_components: Number of components you want to keep.\n",
    "    Output:\n",
    "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
    "    \"\"\"\n",
    "    X_reduced = None\n",
    "    ####################### TODO: Use sklearn.decomposition.PCA to implement this function #####################\n",
    "    X_reduced = sklearn.decomposition.PCA(n_components)\n",
    "    ###########################################################################################################\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "print('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f05e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TODO: Answer the following Question ####################################\n",
    "# Q5: Does the plot above make sense?\n",
    "A5 = \"\"\"\n",
    "Type Yes or No\n",
    "\"\"\"\n",
    "\n",
    "# Q6: Illustrate the plot above\n",
    "A6 = \"\"\"\n",
    "Type your answer here\n",
    "\"\"\"\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22d7959d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mStudent 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStudent_1\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mStudent 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStudent_2\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mQ1:\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mA1\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mQ2:\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mA2\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mPredicted Country:\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcountry_test\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mPredicted Similarity:\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;132;01m{\u001b[39;00msimilarity_test\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124mQ3:\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mA3\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124mQ4:\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mA4\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124mComputed Accuracy:\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124mQ5:\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mA5\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124mQ6:\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mA6\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     34\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(answers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A5' is not defined"
     ]
    }
   ],
   "source": [
    "answers = f\"\"\"\n",
    "Student 1: {Student_1}\n",
    "Student 2: {Student_2}\n",
    "\n",
    "Q1:\n",
    "{A1}\n",
    "\n",
    "Q2:\n",
    "{A2}\n",
    "\n",
    "Predicted Country:\n",
    "{country_test}\n",
    "\n",
    "Predicted Similarity:\n",
    "{similarity_test}\n",
    "\n",
    "Q3:\n",
    "{A3}\n",
    "\n",
    "Q4:\n",
    "{A4}\n",
    "\n",
    "Computed Accuracy:\n",
    "{accuracy}\n",
    "\n",
    "Q5:\n",
    "{A5}\n",
    "\n",
    "Q6:\n",
    "{A6}\n",
    "\"\"\"\n",
    "\n",
    "with open('answers.txt', 'w') as f:\n",
    "    f.write(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
