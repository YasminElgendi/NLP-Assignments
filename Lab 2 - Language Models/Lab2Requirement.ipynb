{"cells":[{"cell_type":"code","execution_count":123,"id":"291c7c1d","metadata":{"id":"291c7c1d"},"outputs":[],"source":["# Student one full name\n","Student1_Name = \"\"\n","\n","# Student two full name\n","Student2_Name = \"\"\n","\n","# team ID\n","team_ID = \"\""]},{"cell_type":"code","execution_count":124,"id":"9818bc3c","metadata":{"id":"9818bc3c"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk import bigrams, FreqDist"]},{"cell_type":"markdown","id":"fa3f0456","metadata":{"id":"fa3f0456"},"source":["# Requirement Description\n","You are given pizza orders dataset. You are required to build some Language Models from the training set. You will then use these models for predicting the masked tokens in the test set. For each order in the test set, a random word was replaced with the word mask. Your task will be to predict the mask work. This requirement is divided into three parts:\n","\n","## Part 1: Data Preprocessing\n","In this part, you will preprocess the given dataset. Once you have preprocessed the orders, you can then use them to build the language models.\n","\n","## Part 2: Bi-gram Language Model\n","In this part, You will build a bi-gram language model from scratch. The Bi-gram language model need to have two main functions: train, predict.\n","\n","## Part 3: RNN Language Model\n","In this part, You will build an RNN language model from scratch. The RNN language model need to have two main functions: train, predict.\n","\n","---\n","Let's get started :D"]},{"cell_type":"code","execution_count":125,"id":"b0a2d337","metadata":{"id":"b0a2d337"},"outputs":[{"name":"stdout","output_type":"stream","text":["train set size = 10000\n","test set size = 1000\n"]}],"source":["# Read the training and test sets\n","train_sentences = pd.read_csv('pizza_train.csv')['text'].to_list()\n","test_sentences = pd.read_csv('masked_pizza_test.csv')['text'].to_list()\n","print(f\"train set size = {len(train_sentences)}\")\n","print(f\"test set size = {len(test_sentences)}\")"]},{"cell_type":"markdown","id":"83a2cf4c","metadata":{"id":"83a2cf4c"},"source":["# Part 1: Data Preproccessing\n","After reading the train and test sets You will do the following steps on both train and test sets:\n","Loop over the sentences to do the following\n","   1. convert all characters to lower (hint: lower())\n","   2. tokenize the sentence (hint: use word_tokenize())\n","   3. Add the start sentence \\<s> and end sentence \\</s> tokens at the beginning and end of each tokenized sentence\n","   4. Add the tokens of the sentence in a predefined set to collect the vocabulary"]},{"cell_type":"code","execution_count":126,"id":"d6148c80","metadata":{"id":"d6148c80"},"outputs":[],"source":["# # This function takes a List of sentences to preprocess them and vocabulary to extend\n","# # It returns a list of sentences after preprocessing and the vocabulary it found\n","# def preprocess(sentences, vocab):\n","#     tokenized_sentences = []\n","#     for i in range(len(sentences)):\n","#         sentence = sentences[i]\n","\n","#         ################################ TODO: Preprocessing  ####################################\n","#         # convert all characters to lower (hint: use lower())\n","\n","\n","#         # tokenize the sentence (hint: use word_tokenize())\n","\n","\n","#         # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n","\n","\n","#         # Add the tokens of the sentence in the predefined set vocab to collect the vocabulary\n","\n","\n","#         # Add the sentence to the tokenized_sentences\n","\n","#         #########################################################################################\n","\n","#     return tokenized_sentences, vocab\n","\n","# This function takes a List of sentences to preprocess them and vocabulary to extend\n","# It returns a list of sentences after preprocessing and the vocabulary it found\n","def preprocess(sentences, vocab):\n","    tokenized_sentences = []\n","    for i in range(len(sentences)):\n","        sentence = sentences[i]\n","\n","        ################################ TODO: Preprocessing  ####################################\n","        # convert all characters to lower (hint: use lower())\n","        sentence = sentence.lower()\n","\n","        # tokenize the sentence (hint: use word_tokenize())\n","        tokens = word_tokenize(sentence,language=\"english\", preserve_line=True)\n","\n","        # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n","        tokens.insert(0,\"<s>\")\n","        tokens.append(\"</s>\")\n","\n","        # Add the tokens of the sentence in the predefined set vocab to collect the vocabulary\n","        vocab.update(tokens)\n","\n","        # Add the sentence to the tokenized_sentences\n","        tokenized_sentences.append(sentence)\n","        #########################################################################################\n","\n","    return tokenized_sentences, vocab"]},{"cell_type":"code","execution_count":127,"id":"1d0f0f04","metadata":{"id":"1d0f0f04"},"outputs":[],"source":["vocab = set()\n","preprocessed_train_sentences, vocab = preprocess(train_sentences, vocab)\n","preprocessed_test_sentences, vocab = preprocess(test_sentences, vocab)\n","vocab.remove(\"mask\")\n","\n","\n","assert type(preprocessed_train_sentences) == list, \"Type of preprocessed train should be list\"\n","assert type(preprocessed_test_sentences) == list, \"Type of preprocessed test should be list\"\n","assert len(preprocessed_train_sentences) == 10000, \"The number of train sentences is not correct\"\n","assert len(preprocessed_test_sentences) == 1000, \"The number of train sentences is not correct\"\n","assert len(vocab) == 307, \"Error in the number of vocabulary\""]},{"cell_type":"markdown","id":"48f2a175","metadata":{"id":"48f2a175"},"source":["# Part two:\n","In this part, You will build an Add one (La place) Smoothed bi-gram language model"]},{"cell_type":"code","execution_count":128,"id":"6e0664c6","metadata":{"id":"6e0664c6"},"outputs":[],"source":["class BigramLM:\n","    \n","    # The class constructor takes the vocabulary\n","    # creates a |V| * |V| numpy 2D matrix for the bi-gram model filled with zeros\n","    # It also creates two dictionaries to be used for token identification\n","    def __init__(self, vocab):\n","        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n","        self.word2id = {word: i for i, word in self.id2word.items()}\n","        self.vocab_size = len(vocab)\n","    \n","        # create a numpy |V| * |V| 2D array filled with zeros\n","        self.CountsMatrix = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n","        \n","    \n","    ####################################### TODO: Complete the BigramLM class ######################################\n","    # This function is responsible for training the Language Model\n","    # It is given the preprocessed training set sentences\n","    # The goal is to fill the 2D matrix with the appropriate counts\n","    # hint: check bigrams() and FreqDist() from nltk\n","    # hint: loop over the sentences to fill the matrix with the appropriate counts\n","    def train(self, train_sentences):\n","        # Get all bigrams possible\n","        bigramWords = []\n","        for trainSentence in train_sentences:\n","            bigramWords += list(bigrams(trainSentence))\n","        # Assign the frequency of bigrams to matrix\n","        freqDist = FreqDist(bigram for bigram in bigramWords)\n","        for bigramWord in bigramWords:\n","            self.CountsMatrix[self.word2id[bigramWord[0]]][self.word2id[bigramWord[1]]] = freqDist[bigramWord]\n","                \n","    \n","    # this function takes two words and calculates the Add-one Smoothed bi-gram probability of it\n","    # Of course the function will make use of the 2D counts matrix built while training\n","    # The function assumes that word1 precedes word2\n","    # The function must return the calculated probability\n","    def calcProbability(self, word1, word2):\n","        num = self.CountsMatrix[self.word2id.get(word1)][self.word2id.get(word2)] + 1\n","        den = sum(self.CountsMatrix[self.word2id[word1]]) + self.vocab_size\n","        #print(num/den)\n","        return num/den\n","    \n","    \n","    # This function takes a preprocessed tokenized sentence with exactly one token = mask (look at the masked test set)\n","    # The function returns a word from the vocabulary that is more likely to be masked\n","    # hint: calculate the probabilities of all possible words in the vocabulary and return the word with the maximum probability\n","    def predict(self, tokenized_sentence):\n","        # Get word before mask \n","        wordIndex = self.word2id[tokenized_sentence[tokenized_sentence.index(\"mask\") - 1]]\n","        # Return the max prob\n","        return self.id2word[np.argmax(self.CountsMatrix[wordIndex])]\n","    \n","    \n","    # This function takes a token and samples the next token\n","    # hint: check np.random.choice\n","    def sample(self, token):\n","        nextTokens = self.CountsMatrix[self.word2id[token]] \n","        #print(nextTokens)\n","        return self.id2word[np.random.choice(nextTokens)]\n","    \n","    \n","    # This function generates a new order using the Bi-gram probabilities\n","    # Returns one string where the generated tokens are white space separated\n","    # Note: The start token <s> and end token </s> shouldn't appear in the generated order\n","    # hint: start the generation with <s> and stop generating tokens when you reach </s>\n","    def generateOrder(self):\n","        generatedOrder = \"\"\n","        nextToken = self.sample(\"<s>\") # sample the next token\n","        \n","        # As long as we didn't reach </s>\n","        # Generate sentences Skipping all <s>\n","        # Seperate with white space\n","        while nextToken != \"</s>\":\n","            while nextToken == \"<s>\": \n","                nextToken = self.sample(nextToken) \n","            generatedOrder += nextToken + \" \" \n","            nextToken = self.sample(nextToken)\n","        return generatedOrder\n","    ################################################################################################################"]},{"cell_type":"markdown","id":"be7b4632","metadata":{"id":"be7b4632"},"source":["# The next cell to make sure your Bigram LM is correct"]},{"cell_type":"code","execution_count":129,"id":"e9cba284","metadata":{"id":"e9cba284"},"outputs":[{"ename":"KeyError","evalue":"' '","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bigramLM \u001b[39m=\u001b[39m BigramLM(vocab)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bigramLM\u001b[39m.\u001b[39mtrain(preprocessed_train_sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m bigramLM\u001b[39m.\u001b[39mcalcProbability(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0.6489202480222365\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProbability Error\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m bigramLM\u001b[39m.\u001b[39mcalcProbability(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0.000299311583358276\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProbability Error\u001b[39m\u001b[39m\"\u001b[39m\n","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m freqDist \u001b[39m=\u001b[39m FreqDist(bigram \u001b[39mfor\u001b[39;00m bigram \u001b[39min\u001b[39;00m bigramWords)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m bigramWord \u001b[39min\u001b[39;00m bigramWords:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCountsMatrix[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[bigramWord[\u001b[39m0\u001b[39m]]][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[bigramWord[\u001b[39m1\u001b[39m]]] \u001b[39m=\u001b[39m freqDist[bigramWord]\n","\u001b[1;31mKeyError\u001b[0m: ' '"]}],"source":["bigramLM = BigramLM(vocab)\n","bigramLM.train(preprocessed_train_sentences)\n","\n","assert bigramLM.calcProbability('i', \"'d\") == 0.6489202480222365, \"Probability Error\"\n","assert bigramLM.calcProbability(\"'d\", \"i\") == 0.000299311583358276, \"Probability Error\"\n","assert bigramLM.predict(['<s>', 'one', 'mask', 'sized', 'green', 'pepper', '</s>']) == 'party', \"Prediction Error\""]},{"cell_type":"markdown","id":"0e67454a","metadata":{"id":"0e67454a"},"source":["# Part three:\n","In this part, you will build and RNN Language model from scratch. This part may be a little bit tricky specially in the back propagation, but we will go trough it step by step"]},{"cell_type":"code","execution_count":null,"id":"d5ead671","metadata":{"id":"d5ead671"},"outputs":[],"source":["class RNNLM:\n","\n","    # The class constructor takes the vocabulary\n","    # creates two dictionaries to be used for token identification\n","    # creates the embeddings matrix (one hot encodings for tokens)\n","    # creates all wights of the RNN Wx, Wh, b, Wy, by\n","    def __init__(self, vocab, hidden_dim=128):\n","        vocab.add('null')\n","        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n","        self.word2id = {word: i for i, word in self.id2word.items()}\n","        self.vocab_size = len(vocab)\n","        self.null_word_index = self.word2id['null']\n","\n","        # Create a numpy |V| * |V| 2D identity array\n","        self.embeddings = np.identity(self.vocab_size)\n","\n","        # Initialize RNN weights\n","        np.random.seed(5)\n","        self.Wx = np.random.randn(self.vocab_size, hidden_dim).astype(np.float64)\n","        self.Wx /= np.sqrt(self.vocab_size)\n","        self.Wh = np.random.randn(hidden_dim, hidden_dim).astype(np.float64)\n","        self.Wh /= np.sqrt(hidden_dim)\n","        self.b = np.zeros(hidden_dim).astype(np.float64)\n","        self.Wy = np.random.randn(hidden_dim, self.vocab_size).astype(np.float64)\n","        self.Wy /= np.sqrt(hidden_dim)\n","        self.by = np.zeros(self.vocab_size).astype(np.float64)\n","\n","\n","    def one_step_forward(self, x, h_prev, Wx, Wh, b):\n","        \"\"\"\n","        This function runs one time step of the RNN. It should implement the RNN equation\n","        that takes input embedding and the previous hidden state then produces the new hidden state.\n","\n","        Inputs:\n","        - x: Input data of the current time step of shape (N, D)\n","             where N is the number of tokens and D is the vocab size\n","        - h_prev: the hidden state from the previous time step of shape (N, H)\n","        - Wx: the weight matrix for input to hidden transformation of shape (D, H)\n","        - Wh: the weight matrix for hidden to hidden tranformation of shape (H, H)\n","        - b: the bias of shape (H,)\n","\n","        Returns a tuple of:\n","        - next_h: Next hidden state of shape (N, H)\n","        - cache: a tupple of all data needed for backpropagation\n","        \"\"\"\n","\n","        next_h, cache = None, None\n","\n","        ############################### TODO: Impelement RNN equation for forward pass ##############################\n","\n","        # use numpy to implement this equation next_h = tanh(Wx*x + Wh*h_prev + b)\n","        next_h = np.tanh(np.dot(x, Wx) + np.dot(h_prev, Wh) + b)\n","\n","\n","        # cache all needed values for backpropagation\n","        cache = (x, h_prev, Wx, Wh, b, next_h)\n","\n","\n","        #############################################################################################################\n","\n","        return next_h, cache\n","\n","\n","    def one_step_backward(self, dnext_h, cache):\n","        \"\"\"\n","        This function implements the backward pass of a single time step RNN\n","\n","        Inputs:\n","        - dnext_h: the gradient of the loss with respect to the next hidden state of shape (N, H)\n","        - cache: a tupple that we cached before from the forward pass\n","\n","        Returns:\n","        - dprev_h: Gradients of previous hidden state, of shape (N, H)\n","        - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n","        - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n","        - db: Gradients of bias vector, of shape (H,)\n","        \"\"\"\n","        dprev_h, dWx, dWh, db = None, None, None, None\n","        x, prev_h, Wx, Wh, b, tanh = cache\n","\n","        ################# TODO: Impelement the backward pass by calculating derivatives and applying chain rule ###########\n","        dtanh = 1 - tanh**2\n","\n","        dprev_h = (dnext_h * dtanh) @ np.transpose(Wh)\n","\n","        dWx = np.transpose(x) @ (dnext_h * dtanh)\n","\n","        dWh = np.transpose(prev_h) @ (dnext_h * dtanh)\n","\n","        db = np.sum(dnext_h * dtanh, axis=0)\n","\n","        ###################################################################################################################\n","\n","        return dprev_h, dWx, dWh, db\n","\n","\n","    def full_forward_pass(self, x, h0, Wx, Wh, b):\n","        \"\"\"\n","        This function runs the RNN forward on an entire sequence of data. We assume an input\n","        sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n","        size of H, and we work over a minibatch containing N sequences. After running\n","        the RNN forward, we return the hidden states for all timesteps.\n","\n","        Inputs:\n","        - x: Input data for the entire timeseries, of shape (N, T, D).\n","        - h0: Initial hidden state, of shape (N, H)\n","        - Wx: Weight matrix for input to hidden transformation, of shape (D, H)\n","        - Wh: Weight matrix for hidden to hidden transformation, of shape (H, H)\n","        - b: Biases of shape (H,)\n","\n","        Returns a tuple of:\n","        - h: Hidden states for the entire timeseries, of shape (N, T, H).\n","        - cache: All values needed in the backward pass\n","        \"\"\"\n","        h, cache = None, None\n","        N, T, D = x.shape\n","        H = h0.shape[1]\n","        #################### TODO: Implement the full forward pass of the RNN ###########################\n","        # Hint 1: You will use the one_step_forward to compute each time step hidden state and cache\n","        # Hint 2: For loop over T and call one_step_forward with the appropriate parameters\n","\n","        # create numpy array filled with zeros of shape (N, T, H) for h to be filled then returned\n","        h = np.zeros((N, T, H))\n","\n","        # initialize the cache as an empty list to be filled with all caches returned from one_step_forward\n","        cache = []\n","\n","        prev_h = h0\n","        # loop over T and calculate the needed hidden states and caches\n","        for i in range(T):\n","            current_h, current_cache = self.one_step_forward(x[:, i, :], prev_h, Wx, Wh, b)\n","            cache.append(current_cache)\n","            h[:, i, :] = current_h\n","            prev_h = current_h\n","\n","\n","        #################################################################################################\n","\n","        return h, cache\n","\n","\n","    def full_backward_pass(self, dh, cache):\n","        \"\"\"\n","        This function computes the backward pass for the RNN over an entire sequence of data.\n","        Inputs:\n","        - dh: The loss gradient with respect to all hidden states, of shape (N, T, H).\n","\n","        Returns a tuple of:\n","        - dh0: Gradient of initial hidden state, of shape (N, H)\n","        - dWx: Gradient of input to hidden weights, of shape (D, H)\n","        - dWh: Gradient of hidden to hidden weights, of shape (H, H)\n","        - db: Gradient of biases, of shape (H,)\n","        \"\"\"\n","        \n","        # dprev_h, dWx, dWh, db\n","        dh0, dWx, dWh, db = None, None, None, None\n","        N, T, H = dh.shape\n","\n","\n","        #################### TODO: Implement the full backward pass of the RNN ###########################\n","        # Hint 1: You will use the one_step_backward to compute each time step gradients\n","        # Hint 2: For loop over T and call one_step_backward with the appropriate parameters\n","        # Hint 3: Don't forget to add dprev_h returned from one_step_backward to the appropriate index in dh while passing\n","        #         to one_step_backward\n","\n","        # adjust the shapes of all returns\n","        D = cache[0][0].shape[1] # get the D dimension (size of vocab)\n","        dWx = np.zeros((D, H))\n","        dWh = np.zeros((H, H))\n","        db = np.zeros(H)\n","\n","        # backwards -> start at the end -> T-1\n","        dh0 = dh[:, T-1, :]  # dh0 = (N, H) and dh = (N, T, H)\n","        \n","        for i in range(T-1, -1, -1):\n","            dh0, dWx_current, dWh_current, db_current = self.one_step_backward(dh0, cache[i])\n","            if i > 0:\n","                dh0 += dh[:, i-1, :]  # add dprev_h returned from one_step_backward to the appropriate index in dh\n","            dWx += dWx_current\n","            dWh += dWh_current\n","            db += db_current\n","\n","        ##################################################################################################\n","\n","        return dh0, dWx, dWh, db\n","\n","    def hidden_to_scores_forward(self, h, Wy, by):\n","        \"\"\"\n","        This function implements the forward pass of transforming the hidden state to the scores vector.\n","        The input is a set of H-dimensional vectors arranged into a minibatch of N timeseries, each of length T.\n","        The goal is transform from the hidden state to the scores output vector with size V (number of vocabulary).\n","\n","        Inputs:\n","        - h: Input data of shape (N, T, H)\n","        - Wy: Weights of shape (H, V)\n","        - by: Biases of shape (V,)\n","\n","        Returns a tuple of:\n","        - y: Output data of shape (N, T, V)\n","        - cache: Values needed for the backward pass\n","        \"\"\"\n","        N, T, H = h.shape\n","        V = by.shape[0]\n","\n","        y, cache = None, None\n","\n","        ##################### TODO: calculate the scores matrix y and cache all needed values ########################\n","        # Hint 1: you need to caculate this formula y = Wy*h + by\n","        # Hint 2: you can reshape h from (N, T, H) to (N*T, H) before the matrix multiplication\n","        #         then reshape it back to (N, T, H) after the matrix multiplication and before adding the by\n","\n","        y = (np.dot(h.reshape(N*T, H), Wy) + by).reshape(N, T, V)\n","        cache = (h, Wy, by, y)\n","\n","        ##############################################################################################################\n","\n","        return y, cache\n","\n","\n","    def hidden_to_scores_backward(self, dy, cache):\n","        \"\"\"\n","        Backward pass for the hidden to scores layer.\n","        Input:\n","        - dy: The gradients of shape (N, T, V)\n","        - cache: Values from forward pass\n","        Returns a tuple of:\n","        - dh: Gradient of input, of shape (N, T, H)\n","        - dWy: Gradient of weights, of shape (H, V)\n","        - dby: Gradient of biases, of shape (V,)\n","        \"\"\"\n","        h, Wy, by, y = cache\n","        N, T, H = h.shape\n","        V = by.shape[0]\n","\n","        dh, dWy, dby = None, None, None\n","\n","        ################################ TODO: calculate the gradients ##############################\n","        # Hint: use reshape to ensure correct dimensions for matrix multiplications and derivatives\n","        dh = np.dot(dy.reshape(N*T, V), np.transpose(Wy)).reshape(N, T, H)\n","        dWy = np.dot(np.transpose(h.reshape(N*T, H)), dy.reshape(N*T, V))\n","        dby = np.sum(dy, axis=(0, 1))\n","\n","        #############################################################################################\n","\n","        return dh, dWy, dby\n","\n","    def softmax_loss(self, x, y, mask):\n","        \"\"\"\n","        This function calculates the softmax loss over minibatches of size N.\n","\n","        Inputs:\n","        - x: Input scores, of shape (N, T, V)\n","        - y: Ground-truth indices, of shape (N, T) where each element is in the range\n","             0 <= y[i, t] < V\n","        - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n","          the scores at x[i, t] should contribute to the loss.\n","        Returns a tuple of:\n","        - loss: Scalar giving loss\n","        - dx: Gradient of loss with respect to scores x, of shape (N, T, V).\n","        \"\"\"\n","        N, T, V = x.shape\n","        x_flat = x.reshape(N * T, V)\n","        y_flat = y.reshape(N * T)\n","        mask_flat = mask.reshape(N * T)\n","        probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n","        probs /= np.sum(probs, axis=1, keepdims=True)\n","        loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n","        dx_flat = probs.copy()\n","        dx_flat[np.arange(N * T), y_flat] -= 1\n","        dx_flat /= N\n","        dx_flat *= mask_flat[:, None]\n","        dx = dx_flat.reshape(N, T, V)\n","        return loss, dx\n","\n","\n","    def word_embeddings_forward(self, x_sentences, W_embedd):\n","        \"\"\"\n","        This function takes input with tokens as indexes to produce their word embeddings\n","\n","        Inputs:\n","        - x_sentences: input sentences of shape (N, T)\n","        - W_embedd: embeddings matrix of size (V, V)\n","\n","        Returns:\n","        - x: matrix with word embeddings of shape (N, T, V)\n","        \"\"\"\n","        return W_embedd[x_sentences]\n","\n","\n","    def loss(self, sentences):\n","        \"\"\"\n","        This function computes the loss for training and the gradients of all RNN weights.\n","\n","        Inputs:\n","        - sentences: Ground-truth sentences; an integer array of shape (N, T) where\n","          each element is in the range 0 <= y[i, t] < V\n","        Returns a tuple of:\n","        - loss: Scalar loss\n","        - grads: Dictionary of gradients for all RNN weights\n","        \"\"\"\n","        # Since we are training a language model, for each token input to the RNN we should predict the\n","        # next token. So the inputs will the all tokens from begin to end -1 and the outputs will be all tokens\n","        # except the first token\n","        sentences_in = sentences[:, :-1]\n","        sentences_out = sentences[:, 1:]\n","\n","        # You'll need this\n","        mask = (sentences_out != self.null_word_index)\n","\n","        # Word embedding matrix\n","        W_embed = self.embeddings\n","\n","        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n","        Wx, Wh, b = self.Wx, self.Wh, self.b\n","\n","        # Weight and bias for the hidden-to-vocab transformation.\n","        Wy, by = self.Wy, self.by\n","\n","        # sizes\n","        N, T = sentences.shape\n","        V, H = Wx.shape\n","\n","        loss, grads = 0.0, {'Wx':None, 'Wh':None, 'b':None, 'Wy':None, 'by':None}\n","\n","        ###################### TODO: Implement the forward and backward passes for the RNN LM #########################\n","        # In the forward pass you will need to do the following:\n","\n","        # (1) Initilize the initial hidden state h0 to a zeros matrix of size (N, H)\n","        h0 = np.zeros((N, H))\n","\n","        # (2) Use a word embedding layer to transform the words in captions_in\n","        #     from indices to vectors, giving an array of shape (N, T, W).\n","        x = self.word_embeddings_forward(sentences_in, W_embed)\n","\n","        # (3) Call rnn_forward with the appropriate parameters to produce\n","        #     array of hidden states with shape (N, T, H)\n","        h, forward_cache = self.full_forward_pass(x, h0, Wx, Wh, b)\n","\n","        # (4) Call hidden_to_scores_forward to get an array of scores of shape (N, T, V)\n","        y, hidden_scores_cache = self.hidden_to_scores_forward(h, Wy, by)\n","\n","        # (5) Call softmax_loss to compute loss using captions_out, ignoring\n","        #     the points where the output word is <NULL> using the mask above.\n","        loss, dy = self.softmax_loss(y, sentences_out, mask)\n","\n","        # In the backward pass you will need to compute the gradient of the loss\n","        # with respect to all model parameters. Use the loss and grads variables\n","        # defined above to store loss and gradients; grads['k'] should give the\n","        # gradients for self..\n","\n","        # (1) Call hidden_to_scores_backward\n","        dh, dWy, dby = self.hidden_to_scores_backward(dy, hidden_scores_cache)\n","\n","        # (2) Call full_backward_pass\n","        dh0, dWx, dWh, db = self.full_backward_pass(dh, forward_cache)\n","\n","        # (3) save the gradients in the grads dictionary\n","        grads['Wx'] = dWx\n","        grads['Wh'] = dWh\n","        grads['b'] = db\n","        grads['Wy'] = dWy\n","        grads['by'] = dby\n","\n","\n","        ###############################################################################################################\n","\n","        return loss, grads\n","\n","    def preprocessDataset(self, train_set):\n","        train_set = [[self.word2id[word] for word in s] for s in train_set]\n","        max_length = np.max([len(s) for s in train_set])\n","        train_set = [s + [self.null_word_index] * (max_length - len(s)) for s in train_set]\n","        return np.array(train_set, dtype=int)\n","\n","    def train(self, sentences, batch_size=64, num_iterations=100, lr=0.0001):\n","        for iteration in range(num_iterations):\n","            index = 0\n","            while index < len(sentences):\n","                loss, grads = self.loss(sentences[index: index + batch_size])\n","                self.Wx -= lr * grads['Wx']\n","                self.Wh -= lr * grads['Wh']\n","                self.b -= lr * grads['b']\n","                self.Wy -= lr * grads['Wy']\n","                self.by -= lr * grads['by']\n","                index += batch_size\n","            print(f\"Iteration: {iteration} | loss = {loss}\")\n","\n","    def predict(self, tokenized_sentence):\n","        idx = tokenized_sentence.index(\"mask\")\n","        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n","        x = self.word_embeddings_forward([self.word2id[token] for token in tokenized_sentence[:idx]], self.embeddings).reshape(1, idx, self.vocab_size)\n","        for i in range(idx):\n","            prev_h, cache_h = self.one_step_forward(x[:,i,:], prev_h, self.Wx, self.Wh, self.b)\n","        out, cache_voc = self.hidden_to_scores_forward(prev_h.reshape(1, 1, -1), self.Wy, self.by)\n","        return self.id2word[np.argmax(out)]\n","\n","\n","    def sample(self, token, prev_h):\n","        \"\"\"\n","        This function takes a token and previous hidden state and samples the next token based on the softmax\n","        Probability distribution\n","\n","        Inputs:\n","        - token: string containing the current token\n","        - prev_h: the previous hidden state of shape (1, H)\n","\n","        Returns:\n","        - next_token: string containing the sampled next token\n","        - curr_h: the produced hidden state from the RNN\n","        \"\"\"\n","        x = self.word_embeddings_forward([self.word2id[token]], self.embeddings).reshape(1, 1, self.vocab_size)\n","        Wx, Wh, b = self.Wx, self.Wh, self.b\n","        Wy, by = self.Wy, self.by\n","\n","        next_token, curr_h = None, None\n","        ############################ TODO: implement the sampling ############################################\n","        # Call one_step_forward with the appropriate parameters\n","        curr_h, forward_cache = self.one_step_forward(x, prev_h, Wx, Wh, b)\n","\n","        # Call hidden_to_scores_forward to produce the scores\n","        y, hidden_scores_cache = self.hidden_to_scores_forward(curr_h, Wy, by)\n","\n","        # Compute the softmax your self. Hint: check np.exp\n","        y = np.exp(y) / np.sum(np.exp(y))\n","\n","        # Sample the token using np.random.choice\n","        next_token = self.id2word[np.random.choice(self.vocab_size, p=y[0,0])]\n","\n","        ######################################################################################################\n","        return next_token, curr_h\n","\n","    def generateOrder(self):\n","        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n","        tokens = []\n","        t = '<s>'\n","        while t != '</s>':\n","            t, prev_h = self.sample(t, prev_h)\n","            tokens.append(t)\n","        return ' '.join(tokens[:-1])"]},{"cell_type":"markdown","id":"33f0e26d","metadata":{"id":"33f0e26d"},"source":["# Let's Test Your Code\n","The next code cell implements functions for testing"]},{"cell_type":"code","execution_count":null,"id":"03b1f075","metadata":{"id":"03b1f075"},"outputs":[],"source":["def rel_error(x, y):\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","    \"\"\"\n","    a naive implementation of numerical gradient of f at x\n","    - f should be a function that takes a single argument\n","    - x is the point (numpy array) to evaluate the gradient at\n","    \"\"\"\n","\n","    fx = f(x) # evaluate function value at original point\n","    grad = np.zeros_like(x)\n","    # iterate over all indexes in x\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","\n","        # evaluate function at x+h\n","        ix = it.multi_index\n","        oldval = x[ix]\n","        x[ix] = oldval + h # increment by h\n","        fxph = f(x) # evalute f(x + h)\n","        x[ix] = oldval - h\n","        fxmh = f(x) # evaluate f(x - h)\n","        x[ix] = oldval # restore\n","\n","        # compute the partial derivative with centered formula\n","        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","        if verbose:\n","            print(ix, grad[ix])\n","        it.iternext() # step to next dimension\n","\n","    return grad\n","\n","\n","def eval_numerical_gradient_array(f, x, df, h=1e-5):\n","    \"\"\"\n","    Evaluate a numeric gradient for a function that accepts a numpy\n","    array and returns a numpy array.\n","    \"\"\"\n","    grad = np.zeros_like(x)\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        ix = it.multi_index\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h\n","        pos = f(x).copy()\n","        x[ix] = oldval - h\n","        neg = f(x).copy()\n","        x[ix] = oldval\n","\n","        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n","        it.iternext()\n","    return grad"]},{"cell_type":"markdown","id":"25645083","metadata":{"id":"25645083"},"source":["# Test One Step Forward"]},{"cell_type":"code","execution_count":null,"id":"ea6e4d37","metadata":{"id":"ea6e4d37"},"outputs":[{"name":"stdout","output_type":"stream","text":["next_h error:  6.292421426471037e-09\n"]}],"source":["N, D, H = 3, 10, 4\n","rnnLM = RNNLM(vocab)\n","x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n","prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n","b = np.linspace(-0.2, 0.4, num=H)\n","\n","next_h, _ = rnnLM.one_step_forward(x, prev_h, Wx, Wh, b)\n","expected_next_h = np.asarray([\n","  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n","  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n","  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n","\n","assert rel_error(expected_next_h, next_h) < 1e-8, \"Error in one_step_forward\"\n","print('next_h error: ', rel_error(expected_next_h, next_h))"]},{"cell_type":"markdown","id":"ea924f7f","metadata":{"id":"ea924f7f"},"source":["# Test One Step Backwad"]},{"cell_type":"code","execution_count":null,"id":"a00b1fb6","metadata":{"id":"a00b1fb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["dprev_h error:  2.732465795967104e-10\n","dWx error:  9.709219069305414e-10\n","dWh error:  5.034262638717296e-10\n","db error:  1.708752322503098e-11\n"]}],"source":["rnnLM = RNNLM(vocab)\n","np.random.seed(231)\n","N, D, H = 4, 5, 6\n","x = np.random.randn(N, D)\n","h = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnnLM.one_step_forward(x, h, Wx, Wh, b)\n","\n","dnext_h = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fh = lambda prev_h: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","fb = lambda b: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n","dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n","\n","dprev_h, dWx, dWh, db = rnnLM.one_step_backward(dnext_h, cache)\n","\n","assert rel_error(dprev_h_num, dprev_h) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(dWx_num, dWx) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(dWh_num, dWh) < 1e-9, \"Error in one_step_backward\"\n","assert rel_error(db_num, db) < 1e-9, \"Error in one_step_backward\"\n","print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"00bc0f72","metadata":{"id":"00bc0f72"},"source":["# Test full forward pass"]},{"cell_type":"code","execution_count":null,"id":"e8ecd42f","metadata":{"id":"e8ecd42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["h error:  7.728466151011529e-08\n"]}],"source":["N, T, D, H = 2, 3, 4, 5\n","rnnLM = RNNLM(vocab)\n","\n","x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n","h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n","b = np.linspace(-0.7, 0.1, num=H)\n","\n","h, _ = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n","expected_h = np.asarray([\n","  [\n","    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n","    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n","    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n","  ],\n","  [\n","    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n","    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n","    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n","assert rel_error(expected_h, h) < 1e-7, \"Error in full forward pass\"\n","print('h error: ', rel_error(expected_h, h))"]},{"cell_type":"markdown","id":"25d21145","metadata":{"id":"25d21145"},"source":["# Test full backward pass"]},{"cell_type":"code","execution_count":null,"id":"b37b6669","metadata":{"id":"b37b6669"},"outputs":[{"name":"stdout","output_type":"stream","text":["dh0 error:  6.635899050005434e-11\n","dWx error:  3.487261230379876e-10\n","dWh error:  7.362820530971359e-10\n","db error:  3.7093984897963457e-11\n"]}],"source":["np.random.seed(231)\n","rnnLM = RNNLM(vocab)\n","N, D, T, H = 2, 3, 10, 5\n","\n","x = np.random.randn(N, T, D)\n","h0 = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","dh0, dWx, dWh, db = rnnLM.full_backward_pass(dout, cache)\n","\n","fx = lambda x: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fh0 = lambda h0: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","fb = lambda b: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n","\n","dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","assert rel_error(dh0_num, dh0) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(dWx_num, dWx) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(dWh_num, dWh) < 1e-9, \"Error in full_backward_pass\"\n","assert rel_error(db_num, db) < 1e-9, \"Error in full_backward_pass\"\n","\n","print('dh0 error: ', rel_error(dh0_num, dh0))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"4420a0c7","metadata":{"id":"4420a0c7"},"source":["# Check hidden to scores forward and backward"]},{"cell_type":"code","execution_count":null,"id":"97aa4935","metadata":{"id":"97aa4935"},"outputs":[{"name":"stdout","output_type":"stream","text":["dx error:  6.848830429905787e-10\n","dw error:  8.503919884552969e-11\n","db error:  7.534133932432161e-12\n"]}],"source":["# Gradient check for temporal affine layer\n","N, T, D, M = 2, 3, 4, 5\n","rnnLM = RNNLM(vocab)\n","x = np.random.randn(N, T, D)\n","w = np.random.randn(D, M)\n","b = np.random.randn(M)\n","\n","out, cache = rnnLM.hidden_to_scores_forward(x, w, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","fw = lambda w: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","fb = lambda b: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dw_num = eval_numerical_gradient_array(fw, w, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","dx, dw, db = rnnLM.hidden_to_scores_backward(dout, cache)\n","\n","assert rel_error(dx_num, dx), \"Error in hidden to scores\"\n","assert rel_error(dw_num, dw), \"Error in hidden to scores\"\n","assert rel_error(db_num, db), \"Error in hidden to scores\"\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dw error: ', rel_error(dw_num, dw))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","id":"d199e23a","metadata":{"id":"d199e23a"},"source":["# Check the loss function"]},{"cell_type":"code","execution_count":null,"id":"0ba3e645","metadata":{"id":"0ba3e645"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wx relative error: 2.436029e-08\n"]},{"name":"stdout","output_type":"stream","text":["Wh relative error: 9.356980e-07\n","b relative error: 1.327157e-08\n","Wy relative error: 3.973081e-08\n","by relative error: 4.594465e-10\n"]}],"source":["N, H = 5, 20\n","test_vocab = {'', 'cat', 'dog'}\n","V = len(test_vocab)\n","T = 10\n","\n","model = RNNLM(test_vocab, H)\n","\n","sentences_test = (np.arange(N * T) % V).reshape(N, T)\n","\n","loss, grads = model.loss(sentences_test)\n","\n","\n","f = lambda _: model.loss(sentences_test)[0]\n","param_grad_num = eval_numerical_gradient(f, model.Wx, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wx'])\n","print('%s relative error: %e' % ('Wx', e))\n","assert e < 1e-6, \"Error in dWx\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.Wh, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wh'])\n","print('%s relative error: %e' % ('Wh', e))\n","assert e < 1e-6, \"Error in dWh\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.b, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['b'])\n","print('%s relative error: %e' % ('b', e))\n","assert e < 1e-6, \"Error in db\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.Wy, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['Wy'])\n","print('%s relative error: %e' % ('Wy', e))\n","assert e < 1e-4, \"Error in dWy\"\n","\n","param_grad_num = eval_numerical_gradient(f, model.by, verbose=False, h=1e-6)\n","e = rel_error(param_grad_num, grads['by'])\n","print('%s relative error: %e' % ('by', e))\n","assert e < 1e-6, \"Error in dby\""]},{"cell_type":"markdown","id":"60c23262","metadata":{"id":"60c23262"},"source":["# The following cells will produce the output files"]},{"cell_type":"code","execution_count":null,"id":"a08d7556","metadata":{"id":"a08d7556"},"outputs":[],"source":["with open(f'{team_ID}.txt', 'w') as f:\n","    f.write(team_ID + '\\n')\n","    f.write(Student1_Name + '\\n')\n","    f.write(Student2_Name + '\\n')"]},{"cell_type":"code","execution_count":null,"id":"4d10eec2","metadata":{"id":"4d10eec2"},"outputs":[{"ename":"KeyError","evalue":"' '","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bigramLM \u001b[39m=\u001b[39m BigramLM(vocab)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bigramLM\u001b[39m.\u001b[39mtrain(preprocessed_train_sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predictions_bigram \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m preprocessed_test_sentences:\n","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m freqDist \u001b[39m=\u001b[39m FreqDist(bigram \u001b[39mfor\u001b[39;00m bigram \u001b[39min\u001b[39;00m bigramWords)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m bigramWord \u001b[39min\u001b[39;00m bigramWords:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X41sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCountsMatrix[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[bigramWord[\u001b[39m0\u001b[39m]]][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[bigramWord[\u001b[39m1\u001b[39m]]] \u001b[39m=\u001b[39m freqDist[bigramWord]\n","\u001b[1;31mKeyError\u001b[0m: ' '"]}],"source":["bigramLM = BigramLM(vocab)\n","bigramLM.train(preprocessed_train_sentences)\n","\n","predictions_bigram = []\n","for sentence in preprocessed_test_sentences:\n","    predictions_bigram.append(bigramLM.predict(sentence))\n","\n","pd.DataFrame({'predictions': predictions_bigram}).to_csv(f'{team_ID}_bigram_predictions.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"6c2ba40e","metadata":{"id":"6c2ba40e"},"outputs":[{"ename":"KeyError","evalue":"' '","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnnLM \u001b[39m=\u001b[39m RNNLM(vocab)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_set_RNN \u001b[39m=\u001b[39m rnnLM\u001b[39m.\u001b[39mpreprocessDataset(preprocessed_train_sentences)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rnnLM\u001b[39m.\u001b[39mtrain(train_set_RNN, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, num_iterations\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predictions_rnn \u001b[39m=\u001b[39m []\n","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=368'>369</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocessDataset\u001b[39m(\u001b[39mself\u001b[39m, train_set):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=369'>370</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=370'>371</a>\u001b[0m     max_length \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax([\u001b[39mlen\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [s \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnull_word_index] \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(s)) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=368'>369</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocessDataset\u001b[39m(\u001b[39mself\u001b[39m, train_set):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=369'>370</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=370'>371</a>\u001b[0m     max_length \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax([\u001b[39mlen\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [s \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnull_word_index] \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(s)) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n","\u001b[1;32mc:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\FourthYear\\FirstSemester\\NLP\\Labs\\Lab2-LanguageModels\\Lab2Requirement.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=368'>369</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocessDataset\u001b[39m(\u001b[39mself\u001b[39m, train_set):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=369'>370</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2id[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=370'>371</a>\u001b[0m     max_length \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax([\u001b[39mlen\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/yazmi/OneDrive/Desktop/Uni/FourthYear/FirstSemester/NLP/Labs/Lab2-LanguageModels/Lab2Requirement.ipynb#X42sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m     train_set \u001b[39m=\u001b[39m [s \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnull_word_index] \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(s)) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m train_set]\n","\u001b[1;31mKeyError\u001b[0m: ' '"]}],"source":["rnnLM = RNNLM(vocab)\n","train_set_RNN = rnnLM.preprocessDataset(preprocessed_train_sentences)\n","rnnLM.train(train_set_RNN, lr=0.01, num_iterations=20)\n","\n","predictions_rnn = []\n","for sentence in preprocessed_test_sentences:\n","    predictions_rnn.append(rnnLM.predict(sentence))\n","\n","pd.DataFrame({'predictions': predictions_rnn}).to_csv(f'{team_ID}_rnn_predictions.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"5c888167","metadata":{"id":"5c888167"},"outputs":[],"source":["gold_labels = pd.read_csv('labels.csv')\n","bigram_predictions = pd.read_csv(f'{team_ID}_bigram_predictions.csv')\n","rnn_predictions = pd.read_csv(f'{team_ID}_rnn_predictions.csv')\n","\n","bigram_acc = np.where(bigram_predictions['predictions'] == gold_labels['labels'], True, False)\n","bigram_acc = np.sum(bigram_acc) / len(bigram_acc)\n","\n","rnn_acc = np.where(rnn_predictions['predictions'] == gold_labels['labels'], True, False)\n","rnn_acc = np.sum(rnn_acc) / len(rnn_acc)\n","\n","print(\"Bigram prediction Accuracy = \", bigram_acc)\n","print(\"RNN prediction Accuracy = \", rnn_acc)"]},{"cell_type":"markdown","id":"48c69d50","metadata":{"id":"48c69d50"},"source":["# For your fun"]},{"cell_type":"code","execution_count":null,"id":"74356f27","metadata":{"id":"74356f27"},"outputs":[],"source":["bigram_generation = []\n","np.random.seed(7)\n","for _ in range(10):\n","    s = bigramLM.generateOrder()\n","    bigram_generation.append(s)\n","    print(s)\n","    print()\n","\n","pd.DataFrame({'generations': bigram_generation}).to_csv(f'{team_ID}_bigram_generations.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"e8c3d102","metadata":{"id":"e8c3d102"},"outputs":[{"name":"stdout","output_type":"stream","text":["soda tea yellow meatball pineaple teas i beans tomatoes pizza pickle thick chorrizo four and pepperonis dish spicy apple crust onion dews black pepsis meatballs vegetarian brocoli just stuffed mountain dr also napolitana toppings bottle pecorino rosemary chickens 13 pepper 2 small barbecue napolitana thin meatball two-liter pepperonis 14 bean fifteen fifteen 11 pellegrino ranch basil balsamic yorker pellegrino liter 200-milliliter pepperoni regular zeroes up peperroni shrimps meatlovers pecorino toppings only 8 fat balzamic everything sized one buffalo ale zeroes null ales 2 style 15 cans mountain 1 olives sodas free an jalapeno combination pork sized 15 meatball tunas topping pulled pellegrinos avoid much artichoke vegetables meat hot 12 fifteen spicy regular extra gluten - an vegetarian black oz 8 jalapenos kalamata pickles five fl cokes hold thirteen sprites peperonni cheese olive meatballs dr coke no only pickles topping all caramelized lettuce large pork mediterranean perriers 12 napolitana pulled mushroom crust pellegrino want cheddar one can onion white chicago 10\n","\n","meat apple ground margarita deep gluten combination crust pork peperonni ups pepperonis jalapeno stuffed four bottle fifteen like cans pellegrino chickens everything mushroom 14 small three dr little oil bacons personal peperroni without pork 200 little chorrizo 500-milliliter not just anchovies 6 meatlovers not pineapple topping fourteen 500-milliliter tunas cauliflower meatlover ranch kalamata large italian all works 6 14 ham fanta onion waters spinach spicy party bacons stuffed all mozarella peperroni tea peperoni any sixteen anchovy peperonis 500-ml hawaiian pecorino ranch spinach only tuna 500-milliliter hate hot fluid spicy bbq anchovies 13 the jalapeno veggie caramelized lover two beef garlic arugula medium new cherry pellegrinos combination hams lover rise can flakes 10 eleven two-liter pickles spiced spinach lettuce 1 lovers shrimps hold ounce peppperonis carrot applewood cumin balsamic 7 tunas ml regular wood flake balsamic dew green jalapenos style an balzamic peas grilled ounce chorrizo diet olives oil not powder - <s> combination fourteen waters fantas sized peperonis - coffee sodas napolitana tea broccoli sausage 7 dish peppperonis american bbq hold large five wood med 12 stuffed pickles salami meatlover also lunch new tuna pecorino pea mushroom extra anchovy bean cokes dew 15 applewood lemon any size free crusts ml pineapples thick doctor milliliter pineaples chorizo 16 onions powder carrot ground pork peas sized tomatoes party an cheese 500 rise artichokes leaves cauliflower napolitan yorker only small italian works 9 meatball ales of pineapple zero bacon black without spinach mediterranean pizza bottle cherry garlic bacons lover <s> jalapenos pellegrinos san thirteen meatlover chicago artichokes seven sausage pesto vegan 7 rise little zero flake apple shrimps wood margherita pork hams ground meatlover not small cherry pepperonis all lovers fanta pizzas jalapenos sausages need mexican an only feta york fifteen barbecue chorrizo pineaples salami pie <s> want fifteen water cumin pellegrino napolitana sausage garlic five med vegetarian cans cherry coffees pepers soda chorizo free liter want soda pickles peper waters 7 200-milliliter olives broccoli fanta shrimps big pizzas alfredo hawaiian big meatball 200 all lemon zeros italian 16 dew 500-ml beef milliliter leaves toppings bit a deep vegetarian carrots sized low large american pellegrinos gluten-free pea sixteen peppperonis deep three ups not hams 12 garlic milliliter bacon gluten 3 little toppings up black peperroni onions need cokes three hot pork deep yorker vegan bbq beans peperronni pepperonis pepperonis soda margarita every one bacons fantas cheeseburger zero without 8 hawaiian 12 mozzarella cheddar i fanta cherry tuna rise powder tomato pizza dr and 8 five anchovy 200-milliliter powder tomato yorker everything sourdough parmesan 15 chicken hate 2 have fanta <s> water chicken mushrooms hold tiny american pineapples gluten-free 16 wood pulled much free ice also dew - ml every dew napolitan pineapple five pepperoni coffees extra parsley pea basil avoid perriers sodas not 200-milliliter 'd beans topping 5 oil cokes can pickle tomato rise want meatball combination fifteen pestos 5 ginger medium of pulled medium nine bay fanta buffalo meat artichoke bottles thin fl basil italian san lot free ricotta two-liter sixteen pineaples olives pepsi tomato mushroom anchovies mexican seven diet all olive lemon the dough chorrizo much spicy 6 fat beef pepers tomato 'd perrier pea med peperroni 10 tea diet\n","\n","'d perrier five bean margherita water meatballs and salami medium high mushroom pies i avoid pizzas pestos chorrizo wood hate 11 regular chicago fluid zeroes oil peperoni hams white peperroni pineapples soda fluid six york can caramelized sixteen tunas sprite grilled american five pie ground hold oregano meatball 15 want thick olive can any cumin 5 cheese style 11 beans sausages keto perrier san barbecue ml perriers 13 mediterranean bay carrot kalamata of peppers sausage peppers ale 4 zeroes applewood veggies leaves liter pestos dew without ranch like mozarella six works just ground fl sixteen olive ales yorker tiny banana peppperonis cherry ounce glaze an sauce garlic peppperonis thin chorizo flakes peperonni 'd meatballs basil mediterranean sauce liter one wood everything 500 three buffalo cokes iced only sprites avoid ground only broccoli stuffed coffee pepers peppers ranch bbq zeros fantas 5 basil mexican ten need leaves garlic chorrizo an oil 200 rise pulled cans 6 chorizo regular jalapeno lettuce peperroni free spicy hold ale zeroes caramelized many peperronni without 500-milliliter shrimps hawaiian ginger big 16 have sprites tomatoes alfredo ups onions cauliflower teas dish hate 'd supreme mushrooms also mediterranean bacons meatlover alfredo the five 5 peperoni hawaiian stuffed feta 2 peperroni extra pestos five little chickens 20 onion many flakes pickle thirteen 15 without oil pies lot anchovy oregano diet thick doctor jalapenos powder five 2 ice white yellow ale cans fantas tomatoes barbecue bay powder ml leaves powder party coffee 1 tiny keto twelve new coffee peperronni mexican apple fried extra bottles veggie balzamic cherry six onions tiny ales gluten-free free 200 sodas lovers sprite spinach 14 shrimp supreme liter pizzas pan fantas everything ales bay 4 med fl vegetables dish black flakes wood new toppings rosemary artichokes napolitan spinach grilled feta rosemary pepsis peas gluten and 20 eleven everything medium liter\n","\n","veggies two 11 every banana brocoli cheeseburger med artichokes powder green crusts 200-milliliter hawaiian 15 ml olives cumin one tea veggies peperoni lemon 10 onions dews <s> can keto zeros pesto coffee two pineaple 16 ale fl lettuce vegetarian null 6 8 balsamic 11 margarita veggie ranch zero tomato party napolitana diet seven mushroom garlic chickens pizza meatballs fantas bottle pesto pulled crust rise pellegrinos pan and liter three-liter crusts jalapenos pecorino pie margarita mushroom peppperonis style 200 without san six pepsis pepsi coffees ales margarita much cauliflower deepdish chickens spinach ranch extra 500-milliliter chicken dish high mexican carrot the pineaple dews diet napolitan sprites a wood sausage oregano peas 14 coffees pineaples like 8 8 napolitana 8 veggie with arugula barbecue red small peas peppperoni 6 olives doctor onion beef coffee pulled avoid san and parmesan artichokes veggie dish wood ginger beef want pepsis toppings pineaple in vegetarian fried ales beans just rosemary fantas roasted pepsis need mozarella 3 fourteen cauliflower pea sprite much crust sprite peperronni 8 med crust parmesan 3 pepsis without thick size pie onions 7 want nine all sourdough beans cheddar roasted only two vegan pork pepper four roasted ham iced 12 bay pellegrino perriers oz yorker doctor want stuffed tea three meatlover sized hawaiian chicago tomatoes 2 pepsi free chickens large mexican onion everything fried med style chicken sauce 3 diet chicken ice 3 parsley five lovers hate peppperonis sauce tomato sized thick <s> brocoli tuna 4 peper pickles flakes yorker water thin veggies peperoni tiny gluten fluid ups roasted glaze not pizzas gluten four ham mozarella two brocoli regular can garlic dews avoid anchovies beans 4 meat tiny need bottles everything fluid chicken beans wood pecorino lemon doctor gluten york hawaiian many a 200-milliliter perriers ml works ups grilled yellow soda cheese deepdish peppers bottles with san vegan meatball ale pizzas peppers bit fifteen iced cheese all pepperonis pesto water flakes ten stuffed sixteen sixteen tomato supreme pellegrino gluten null 11 size hot mozzarella ups pie perrier high peas diet keto pizzas fantas lettuce balzamic napolitan bottles gluten pineaples eleven 5 chicago pickles 200-milliliter pulled peas 7 pepsi chorizo peperonni eight cokes teas perrier tomato margherita carrot artichokes water chicken peperroni arugula shrimps ales bacon ground med ranch veggie brocoli pan peperonis supreme hold ginger crust pepsi powder hams napolitana dough ale ham low ounce black much bbq much pesto spinach thirteen meatball artichoke two seven parmesan regular balsamic gluten have large ten without extra beans <s> pan jalapenos mountain crusts cumin an large stuffed ranch thick stuffed mediterranean deep rosemary one deepdish rosemary tea apple ales\n","\n","vegetables pineapple vegan wood bottle works 8 chickens lovers lover 1 500 cumin shrimp 5 pepsi banana 500-ml tiny 12 meatballs pineapple italian stuffed stuffed stuffed chicago need garlic american balsamic little feta leaves ten tomatoes of peppperoni liter 16 yorker only parmesan shrimps 200-milliliter american 200-milliliter peppperonis ups hot shrimps all lovers dough mexican napolitana carrots pan 12 spiced cheddar big 200-milliliter balsamic beef high med lemon topping pan meatlovers bottles gluten low cumin dish napolitana and glaze ranch big bacon vegan cauliflower hold diet artichoke pies dough coffee deepdish 10 six veggies 15 five pie large dr 13 8 7 grilled napolitan anchovies cans pepsi ground parmesan cumin four new white perriers pies meat 11 tunas four low wood - peperonis american rosemary york peperonis peperonis 7 bay liter broccoli ranch chickens white 3 dews sauce 15 spiced only neapolitan med doctor peperonis cheese two have pepperoni four pineaples san in chorizo spinach sprites pan vegetables powder lovers 4 personal a liter balzamic bottle deep milliliter spicy spiced have dough lover artichokes ups every pickles waters zero stuffed up dews 500-ml yorker ml ham not ten peperonni thin ham artichokes big zero cauliflower san peper artichokes barbecue little 16 any sourdough balsamic bit two pecorino stuffed pepperoni peppers sauce sprite cheeseburger cauliflower bottles carrots lovers napolitan kalamata seven cheddar not cherry just cans ale pineapples caramelized 2 ml garlic cheese cherry kalamata peppperonis fat ups pepsi dews party grilled ale veggies basil pepperoni veggies peperonni pecorino oregano ale large barbecue pizzas diet tiny a balsamic 500-milliliter arugula zeroes oregano peperonni leaves ales 1 with pineapple garlic pickles 500 11 20 just hot low the green combination garlic peperonni the peppperoni parsley ten margarita eight have every toppings doctor spiced fanta two-liter deepdish\n","\n","hot doctor ice 10 cheese sprite gluten-free cheeseburger little lots peperroni i tunas bacons stuffed salami yorker ice fat lover sausages 8 cheddar 7 four pies tuna jalapenos beans bacons mushrooms pickles chickens much broccoli pestos 16 meatlovers pellegrinos mushrooms 6 15 want olives pellegrino ml not spicy - 10 pickle 12 napolitana neapolitan balsamic all meat fanta of roasted sausages three sodas powder pepsis topping fantas six cans up anchovy bean 16 sized hold 12 thick diet banana only fourteen pea stuffed peperonis red pellegrinos coke pellegrinos the white i 200 pizzas can chorizo flake vegetables toppings up - three artichokes sized mushrooms meat bbq chicken water brocoli cheese an fl bacon 6 bit zeroes san shrimps need 16 flake olive keto sausages rise doctor artichoke pepsis gluten 'd balsamic null ham little low zeros milliliter feta seven pineapple zero of arugula pulled ml zero cheddar combination in shrimps deepdish hawaiian peperonis olives dews powder leaves free ginger coffee artichokes sized peperroni margarita six white just ground like peperroni veggies mozarella tuna 9 sauce thick any pea alfredo topping beans oz pellegrino dish pineaples twelve need buffalo personal sauce broccoli ricotta lot twelve regular extra i cherry napolitan fluid anchovies the cheddar pestos cumin fat anchovies parmesan buffalo vegan free crusts perriers gluten pepperonis regular 20 every big 7 white with 'd neapolitan salami\n","\n","deep tomatoes pineaples bottle lots hate artichokes sprites peppers and - large mexican <s> pie tunas thirteen barbecue cauliflower tomato mushroom medium american wood artichoke ricotta spicy cans pan water chicago carrots i three pineaple fl eleven pepers american a soda 200-milliliter sausages party tomato meatlovers large rise oregano pineapples hot 3 pies party free lovers peperonni red olives oz ounce meat eleven ground pepers carrot zeros pickles parmesan chicago 9 cans fantas ounce four anchovies fourteen rosemary flake shrimp 16 a pepperoni and little toppings without cheddar hate pepsis lots pineaple thick supreme meatlovers eight roasted bean low peper brocoli extra just high mozarella banana coffees peppers sourdough two-liter four tuna lemon pea ales low all 10 deepdish sixteen fifteen vegetarian cheddar lemon hate pepers like iced eight nine peppperonis artichokes pestos chicago dew pickle high beef onion fantas york jalapeno peperroni fried just cumin rosemary and jalapenos 13 pepper hot carrots pineapple 6 14 no ginger banana mozarella artichoke 7 mountain spicy no broccoli caramelized chicago every of napolitan also anchovy spicy sausage margherita high nine eleven pepers tea olives arugula pineapples 13 broccoli 11 eight lunch peas little small bay mushroom not medium pesto lunch chorrizo everything low medium keto regular beef apple neapolitan avoid 500 doctor rise tiny high dough meatlovers a ups lovers pineapples bacons yellow cans cans ham veggies 2 lots pepsis ups oz cheese veggies ricotta chickens need mexican perrier 2 coke artichoke pepper can 5 pestos mozarella nine meatballs shrimps bacons no soda mediterranean pecorino sausage garlic mountain deepdish salami all pesto 9 cheddar veggie four peppperoni soda 4 jalapeno peperonis dews need green hold peper 20 thin ricotta lover carrots oregano green eleven lots balzamic coffees ml lovers cheeseburger balsamic leaves pesto lot three-liter soda sixteen thick glaze peas ricotta wood parmesan onions gluten-free pellegrinos parsley pepper cauliflower cheeseburger seven lot lemon banana apple meatball peperoni gluten mountain soda italian peperroni pesto small also italian margarita peperroni yorker balzamic coffee 13 ice all ham feta <s> lettuce fourteen deep vegetables flake tomatoes carrots works ups italian oz pickle kalamata dew broccoli 7 fanta mozzarella glaze peperronni little lemon can fl 'd with applewood powder tomato pineaple peppers many anchovy peperonis pan keto meatlovers cheddar dish 15 onions 3 bacons thin with and cheese olive new 13 pork 11 powder basil glaze oz gluten-free oil ricotta ale perriers four alfredo dish flakes mozarella zero oil medium zeroes keto much peppperonis seven new crusts fluid meat spiced can many pestos tuna dough pepers fat 5 combination dough hate parsley pecorino pepsi milliliter arugula thin dried sprites seven applewood york two-liter new 1 medium pineaples shrimps pepsis onion 5 spicy pork parmesan and extra brocoli pellegrino beef ranch peppers zero roasted peperroni rise 6 can balzamic supreme twelve no banana flakes want mexican null peas black 'd small vegan 16 rise green bbq fried peas 15 ale sprite tunas dr perrier and 200 peperonni without mozzarella peperonis not spinach no banana bottle margarita vegetables medium mozzarella dew ground lovers hawaiian napolitan tea 5 bay - coffee fl banana three 7 can and three-liter meat the neapolitan milliliter 16 mountain zeros rise mushroom american 16 three-liter fourteen oil much peper pepperonis bacons black mozzarella cheeseburger water apple caramelized coffees veggies bacons jalapenos need combination with ten caramelized toppings red jalapenos tomatoes pepperonis two pizzas mozarella bean of cans 500-ml 10 sized sourdough pea peper 5 bottles buffalo hate pineapple sized coffee pineapples cokes shrimps 14 bbq eight sourdough without hams not broccoli ales teas arugula 15 mountain onions pickle spinach pestos broccoli vegetarian shrimps cheddar meatball lettuce little arugula stuffed crust peppers cheeseburger 500 not york spicy basil tea 2 artichokes chorrizo sprite tomato pellegrinos pepperoni pies chicago bean oil peperoni peperonis coffee chorizo buffalo sausages dough two-liter personal garlic fifteen little chicago 500-ml pineaple grilled avoid twelve need peperronni ounce ice bacon ranch three-liter spiced artichoke carrot need style and brocoli fried rise peperronni pulled 'd crusts barbecue ricotta sauce wood gluten-free an thirteen hate rise chicago fourteen topping jalapeno large 500-ml mountain toppings 14 fat spinach personal cauliflower cheese hold peppperoni pickle bay hot dried medium ricotta parmesan tiny meatball pellegrino doctor up pesto like deep 1 6 apple chorrizo fifteen toppings carrot cumin 11 kalamata peperronni no four dr pepers meat fanta any oregano want and eleven rise ricotta free leaves - pepsi milliliter pepperonis sodas margarita medium parsley new meatlovers style gluten five oregano sausage pineaple pizza feta wood cherry ground pies four deepdish shrimps bean doctor i olives balsamic style fourteen pizzas sized 16 12 buffalo buffalo cans napolitan olives peperoni meat teas 4 peppers caramelized balsamic deep peperonni 13 eight bean of oil sauce lot extra 9 parmesan yellow five thin pepers toppings pesto sauce pecorino ham 5 chickens high spinach napolitan oil teas gluten-free pesto zero pies beef pecorino salami parsley brocoli ale leaves bit fantas ricotta bit ups size sized tea shrimp 14 size grilled pies chicken napolitana and peppers pie pork napolitana beef lettuce fantas cheese fluid liter onions four oregano doctor peperronni leaves cauliflower med thin 5 mushroom perriers peperroni crusts peperonis cheese mountain yellow avoid apple 200-milliliter crusts pecorino 11 7 beans not stuffed barbecue alfredo onion topping pepsi peperoni sized neapolitan leaves chorrizo four tea cumin sourdough ounce 7 8 high dew new soda new soda zero 9 gluten-free mozarella zeros null 500-ml three-liter two want sourdough pepsi rise cumin ground of meat everything feta fried glaze 13 zero garlic spinach zeros of pecorino peperoni chickens crust 500-ml little peas 8 tiny rosemary personal seven napolitana fried pineaples pesto sodas pineaples diet cokes cauliflower 7 caramelized grilled ale and many balzamic pepper mexican med sodas coffee topping nine dr vegan dew vegan pestos black small ten cheeseburger beef everything parmesan can pies buffalo zeros onion peperoni parmesan five sprite pestos beans ales supreme york chicago peper oregano pepsi deepdish fried chickens teas spinach cheddar bottle with grilled 5 pickles lover vegetarian pecorino mozarella 200-milliliter american fl pan tea the mountain parsley 500-milliliter spicy zeroes veggie low fanta chorrizo thirteen medium hams applewood glaze yellow lettuce ounce perrier pepperoni pepper bit 'd spicy margarita fantas olives pellegrinos peas pineaple pizza pepperonis lemon tiny sized olives waters hold 6 cheeseburger spicy peppperonis bacon want sodas olives buffalo bacon dish the med shrimps hams sauce teas stuffed three 20 pie 9 flakes deep size 11 9 beef dews in bean 8 ground lunch balsamic water dried flake dew mozzarella pie mediterranean tunas glaze 500-milliliter coke red vegetarian glaze carrots chicken ginger lot neapolitan hold ricotta peppers vegan hams onion chicago onion dough pickle hot eight york peper hams without any banana ten medium ale ales peppperonis meatlover perriers broccoli party pepper flake artichoke neapolitan zeros oz supreme olives fifteen three-liter cauliflower teas everything crusts cheeseburger dish balsamic ale small mexican topping of margarita sausages i have doctor - fanta chicken dr a parsley ice ten 20 napolitan <s> sauce regular dews york six white pecorino sausage meatlovers eight fried milliliter san ale party broccoli up pesto ml cheddar pepper spinach broccoli glaze peppperoni onion carrots 10 12 pie mediterranean pork deepdish spiced pulled with chicago pepers neapolitan sixteen pellegrinos black pizzas spinach pestos parmesan three-liter 14 8 coffee sourdough white nine chickens bit flakes toppings caramelized mushrooms sprites veggie alfredo meatball margarita lemon can the meatlovers chicken fat garlic beans neapolitan york cans pepperonis parsley large coffee 1 artichokes dried 'd want vegetables sausages 14 avoid pies meatball mozzarella like mexican thick small hot basil ounce liter garlic hams napolitan neapolitan oregano chorrizo pellegrinos margarita basil combination new meatlover only free anchovy rosemary zeros salami dried 200 garlic meatball combination roasted sausages bbq not i iced want water italian 'd basil sodas 11 vegetables sprites applewood chicago five pineaple much pineaples pies lover fl sodas perrier pork crust dough peperonis shrimp san kalamata spicy every pineapple vegan fourteen meatlover sized want thirteen three-liter arugula lover 6 topping 16 meatlovers liter 14 sausages tunas medium meatball applewood brocoli mushroom the bacons black pulled barbecue five cans vegan tunas pepperonis onion 8 pellegrino low chicken 6 feta lunch medium pizza any like works powder lemon olives pineapples tomatoes tiny 20 applewood pea peperonis basil every fanta rosemary peperronni big mozzarella can barbecue pickle a null chorizo 5 bottle parsley tuna black balzamic red style lemon extra 500-milliliter thin in bacons mexican yorker shrimp 12 vegetables salami everything bacon style lettuce zero carrots avoid anchovy pestos neapolitan topping little pizza neapolitan an everything spicy rise fried crusts powder fanta pan chorizo free sauce yellow olive mushroom pineapples style pulled peppers style mountain pizza mexican sausage wood yellow high deepdish 10 pepperonis much lot big cheese artichoke pickle seven american oregano jalapenos all meatball cheese oregano sausages cherry oz cheddar yellow chorrizo 13 basil two arugula chorizo med chorizo glaze bottles grilled garlic salami meatballs bit york carrots 14 4 brocoli 9 thin broccoli deepdish meatlovers three want black bean sausages zero soda oz much cherry the roasted fanta 500-ml mozzarella shrimps 15 liter onions personal ranch oz i lover can <s> spiced tunas 8 hawaiian pea bay applewood fat jalapenos napolitana 3 pickles fluid cherry margarita works tunas spiced mediterranean bbq cumin any dish ales lunch mozzarella hold in pie\n","\n","rise oz 10 can beef banana just jalapenos 1 vegan tuna many 16 barbecue soda sausages yellow onions just bean wood lettuce shrimp coke all caramelized rosemary beans 9 just dough fourteen med mediterranean dr meatballs up lots bay 5 neapolitan sprite i null pecorino oil chickens cauliflower flakes vegetarian sauce size pie peper buffalo beans 500 12 yellow sourdough mexican can tuna meatlover peper 200 three three-liter up an meatlover pineapples toppings deepdish little everything bean white bottle meat just ginger three-liter tuna a peppers sprites much ginger chorizo white bottles vegan dried mushrooms gluten-free bottle 1 york balzamic doctor pineaples only white artichokes crusts a two-liter not lettuce pineaple lots i\n","\n","fantas bay two-liter fl med yellow ml avoid zero fanta soda supreme gluten fried lettuce crusts peppperonis null no fried big 2 san vegetarian deep shrimp 200 powder 15 lettuce pulled doctor teas 500 jalapeno high six lemon vegan shrimp 5 white gluten-free only toppings 2 napolitana red tunas deepdish extra nine zero alfredo two yorker diet artichokes ml pesto yellow peperonni 20 perriers pesto soda tuna sauce bacons yorker crusts 10 13 cumin pea shrimps low napolitana sixteen bacons onions feta pineapple margarita sauce balzamic milliliter tunas american kalamata thin 500-milliliter coffees no lettuce pepper peperronni meatball perriers pineaple cherry chorizo hams 9 chorizo 6 6 500-ml pepperonis no lettuce bottles alfredo chickens pepers leaves five deepdish ten have 12 garlic pestos pepers crust hold eight artichokes apple tunas dews red tunas 7 pulled mexican 5 cauliflower green mediterranean napolitana every meatball up peperroni carrot tunas one mexican deepdish seven ham iced small crusts ml crust artichoke coffee just chorizo napolitana 500-milliliter eight beef meatball mexican avoid three-liter oil 5 ricotta peas salami dough hawaiian tomato much zeroes cokes ten cans peperronni neapolitan sixteen waters cumin many ml i spinach pepsi 2 mozzarella nine shrimp rosemary ice mexican thirteen pellegrino eleven two peas ham iced shrimp apple york 5 have cokes bbq also applewood gluten personal roasted fanta cans ranch vegetables fourteen spicy the york perriers yellow peas carrot eleven oz ales flake chicago margarita zeroes veggies thirteen rise mountain bottles anchovy cumin 200 spicy parsley 5 8 null 500-ml jalapeno chickens null mozzarella meatlovers personal roasted toppings cauliflower parmesan three fanta bean fanta pepsis 14 anchovies bbq 1 free iced supreme four anchovy two-liter flake peper everything anchovies cheese fanta coke spinach fl spiced garlic flakes flakes sized salami peppers crust veggies and red every meatballs meat sausages oil thick fl liter eleven mexican doctor alfredo italian peper many 10 stuffed peperronni salami pickle with topping barbecue green water hawaiian ales onions three pepper pickles 'd margarita seven artichoke vegetarian 500 anchovy 20 pickle ground mushrooms hate avoid ham works flakes bit eleven like thin bacons with high sixteen lovers cauliflower caramelized beans napolitana mushroom sourdough ginger peperonis ten pepsis perriers soda peperronni eight 500-ml pellegrinos fat banana pulled 'd thick 9 yorker thin regular basil five deep garlic american bean pineapples topping med meatlover york of ales oil meatballs meatlovers bean margherita ups coffees dough of dried - vegetarian pepperoni diet pickles mountain parsley lettuce bbq ml banana napolitana rosemary five <s> peperronni 4 tiny dr with iced pickle buffalo shrimps pepsis keto beans pies broccoli pepper pellegrinos onion peppperonis pan apple much zeroes pineapple pellegrinos dough ginger in beef 10 nine tea mozzarella caramelized glaze tunas the peperronni banana onions soda alfredo toppings two chicken mozzarella like topping without peperronni lot 14 little chorrizo big sausages bean toppings chorrizo fourteen the waters iced peppperonis 2 applewood avoid cherry lots mushroom sourdough chicago thin shrimp low low tomatoes pies ginger deep meatlover meatball yellow napolitana cauliflower liter mozarella basil italian salami peperonis pickles milliliter veggies cheeseburger thick grilled combination 7 banana beef toppings coffee ml size gluten-free thick tomato cheddar ten bottles cans and peppperonis onions hate new peperroni fried nine lover peperronni tuna combination coke sauce beef anchovies 8 leaves many zero hate ales meatball eight 1 just brocoli 6 2 red mushrooms lemon thick tunas extra vegetables 500 pesto sprites bottles crusts everything pineaple peperroni jalapeno avoid 500 2 pea pineaple pellegrinos fifteen pies ginger party size - coke bit avoid pizza broccoli every pork mediterranean pepsis rosemary 7 veggie garlic crust pestos supreme ginger can supreme sourdough chicago fanta sausages waters peperonis only buffalo i six beans fantas cherry pea pepsi mozarella coffee supreme need margarita tomatoes dews regular lemon bbq lettuce up small kalamata artichoke toppings also everything like water cokes 1 perrier 3 tomato shrimps zero olive mediterranean bean peper alfredo york soda twelve iced cauliflower pepers chorizo up parmesan all spinach have veggies pie combination ale dew margherita dew mediterranean jalapenos wood meatlovers margherita tea also chorizo keto chicken roasted coffee six crusts 200 sausage i anchovies 4 coffee two-liter chorrizo garlic little doctor personal pies also pellegrinos <s> margarita of three ginger three liter new dried pesto ham 15 500-ml milliliter bottle lettuce - basil tomato zeros liter fanta cokes pies rosemary italian hate hate pepsi red artichokes lemon pineaples 5 onions seven soda milliliter oregano a meatball cheese powder extra everything shrimps 10 large tiny the need ales coke dews meatlover party personal olives water lover ales crust fat dr peppperoni onion bacons thick pickle sprite much ground pepperoni sized roasted 'd bottle five 13 like oz spinach low mexican soda medium fried flake tuna ale lunch lover gluten-free 11 perriers without roasted hate peas apple olives pineapples null no vegetarian pineaples peper dish lover bean stuffed zeros 4 balsamic fanta chicken brocoli four artichokes thin peperronni bacons sausage sourdough chorizo coke thirteen san works twelve all regular 13 banana ml ounce lots combination null peper 5 6 deepdish extra oz hams red dew fat hawaiian italian 1 pesto dew have mozzarella banana rise oz olives glaze many 13 two-liter ground 200-milliliter peppperonis sourdough pesto pie beans meatballs 7 small all york parsley peperroni 1 doctor fantas beef pellegrinos big meatballs caramelized with ginger like six 200 roasted cans meatlover 2 brocoli lot red cokes cheese hate any glaze chickens meatballs meat jalapeno peperroni liter high new veggies 8\n","\n","coke pulled six zero cheddar sausages pizzas lettuce green fourteen bit napolitana one meat bottles rosemary vegan spiced dew small ups need mexican rosemary coffees broccoli\n","\n"]}],"source":["rnn_generation = []\n","np.random.seed(7)\n","for _ in range(10):\n","    s = rnnLM.generateOrder()\n","    rnn_generation.append(s)\n","    print(s)\n","    print()\n","\n","pd.DataFrame({'generations': rnn_generation}).to_csv(f'{team_ID}_rnn_generations.csv', index=False)"]},{"cell_type":"markdown","id":"9e370b33","metadata":{"id":"9e370b33"},"source":["# Thank you for your efforts :D"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
